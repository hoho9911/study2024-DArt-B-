{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1-1. 데이터 차원 여부 확인"
      ],
      "metadata": {
        "id": "sykudSgRnycD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 변수(특징) 개수 확인"
      ],
      "metadata": {
        "id": "2rXedGqdofrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1)\n",
        "\n",
        "print(f\"데이터 샘플 수 (행): {data.shape[0]}\")\n",
        "print(f\"데이터 변수 수 (열): {data.shape[1]}\")\n",
        "\n",
        "if data.shape[1] > 20:  # 변수 20개 이상을 고차원으로 간주\n",
        "    print(\"데이터는 고차원 데이터입니다.\")\n",
        "else:\n",
        "    print(\"데이터는 저차원 데이터입니다.\")\n"
      ],
      "metadata": {
        "id": "dNmpOyquSO4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 예제 데이터\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3],\n",
        "    'Feature2': [4, 5, 6],\n",
        "    'Feature3': [7, 8, 9],\n",
        "    'Feature4': [10, 11, 12],\n",
        "    'Feature5': [13, 14, 15]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 데이터 크기 출력\n",
        "print(f\"데이터 크기: {df.shape}\")  # (샘플 수, 특성 수)\n",
        "\n",
        "# 샘플 대비 특성 비율 확인\n",
        "num_samples, num_features = df.shape\n",
        "if num_features > num_samples:\n",
        "    print(\"고차원 데이터입니다.\")\n",
        "else:\n",
        "    print(\"고차원이 아닙니다.\")\n"
      ],
      "metadata": {
        "id": "UFBiG1SZSPq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 상관관계 분석"
      ],
      "metadata": {
        "id": "XEiJUfHookrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 상관행렬 계산\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "# 상관행렬 시각화\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# 상관계수 확인\n",
        "high_corr = (abs(corr_matrix) > 0.8).sum().sum() - len(corr_matrix)\n",
        "print(f\"상관계수가 0.8 이상인 변수 쌍 개수: {high_corr}\")\n"
      ],
      "metadata": {
        "id": "RRiNd8NcSPod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 샘플 밀도\n",
        "\n",
        "-> 샘플이 전체 공간에서 희소하면 고차원"
      ],
      "metadata": {
        "id": "teSS7rACosUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import numpy as np\n",
        "\n",
        "# 샘플 간 유클리드 거리 계산\n",
        "distances = euclidean_distances(data)\n",
        "avg_distance = np.mean(distances)\n",
        "\n",
        "print(f\"샘플 간 평균 거리: {avg_distance:.2f}\")\n",
        "if avg_distance > 1.0:\n",
        "    print(\"고차원 데이터로 인해 희소성이 나타날 가능성이 있습니다.\")\n",
        "else:\n",
        "    print(\"데이터가 고차원적이지 않습니다.\")\n"
      ],
      "metadata": {
        "id": "bvX3QqmKSPl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*\n",
        "군집의 밀도와 모양이 구형이 아닌 경우, 밀도 기반 클러스터링(DBSCAN)을 사용하여 분석.\n",
        "DBSCAN은 구형이 아닌 군집도 효과적으로 탐지합니다.\n",
        "DBSCAN 결과와 비교하여 구형 클러스터링이 적절한지 판단."
      ],
      "metadata": {
        "id": "HVWasSRGvvzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-2. 군집형태가 구형인지 파악\n",
        "\n"
      ],
      "metadata": {
        "id": "EDzCkMqUv5J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 구형 : k-means\n",
        "\n",
        "* 아니면 DBSCAN"
      ],
      "metadata": {
        "id": "nNz79RLuwJ9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화 - 2D / 3D 시각화\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCA로 차원 축소\n",
        "pca = PCA(n_components=2)\n",
        "pca_data = pca.fit_transform(data)\n",
        "\n",
        "# 클러스터링 결과 시각화 (예: K-Means 결과)\n",
        "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis')\n",
        "plt.title(\"Cluster Visualization (PCA Reduced)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CHB0v7KowTJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화 - Pairplot 시각화\n",
        "\n",
        "import seaborn as sns\n",
        "sns.pairplot(data.assign(Cluster=clusters), hue=\"Cluster\", diag_kind=\"kde\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7MwLcnwjwid4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 군집 거리 분석-  Silhouette Score\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "sil_score = silhouette_score(data, clusters)\n",
        "print(f\"Silhouette Score: {sil_score:.2f}\")"
      ],
      "metadata": {
        "id": "aIpiUCQswibX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 군집 내 분산 비교\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 각 군집의 중심과 데이터 거리 계산\n",
        "centroids = kmeans.cluster_centers_\n",
        "distances = [np.linalg.norm(data[clusters == i] - centroids[i], axis=1) for i in range(len(centroids))]\n",
        "\n",
        "# 평균 거리 출력\n",
        "avg_distances = [np.mean(dist) for dist in distances]\n",
        "print(f\"군집별 평균 거리: {avg_distances}\")\n"
      ],
      "metadata": {
        "id": "FngjJ96uwiZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Ellipticity 검사(이거 쓰면 빠르지 않을까..??) -> 타원형 군집의 중심과의 거리가 일정하지 않다면 구형x\n",
        "\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 공분산 행렬\n",
        "cov_matrix = np.cov(data.T)\n",
        "\n",
        "# Mahalanobis 거리 계산 (예: 첫 군집)\n",
        "center = centroids[0]\n",
        "mahalanobis_distances = [mahalanobis(point, center, np.linalg.inv(cov_matrix)) for point in data[clusters == 0]]\n",
        "\n",
        "# 평균 거리 출력\n",
        "print(f\"Mahalanobis Distance 평균: {np.mean(mahalanobis_distances):.2f}\")\n"
      ],
      "metadata": {
        "id": "BmvoVDxFwX1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-1 데이터가 저차원??"
      ],
      "metadata": {
        "id": "lXPhZC6asH9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주의 사항 1. 데이터 스케일링하기\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "m4TI7k3AuQDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주의 사항 2. 이상치 확인하기\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "# 주의 사항 2. 이상치 확인하기\n",
        "def detect_outliers(df, features):\n",
        "    outlier_indices = []\n",
        "\n",
        "    for col in features:\n",
        "        # IQR 기반 이상치 탐지\n",
        "        Q1 = np.percentile(df[col], 25)\n",
        "        Q3 = np.percentile(df[col], 75)\n",
        "        IQR = Q3 - Q1\n",
        "        outlier_step = 1.5 * IQR\n",
        "\n",
        "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
        "        outlier_indices.extend(outlier_list_col)\n",
        "\n",
        "    outlier_indices = sorted(list(set(outlier_indices)))\n",
        "    return outlier_indices\n",
        "\n",
        "# 이상치 탐지 실행 (모든 열에 대해)\n",
        "outlier_indices = detect_outliers(df, df.columns)\n",
        "\n",
        "# 이상치 출력\n",
        "print(\"이상치 인덱스:\", outlier_indices)\n",
        "\n",
        "# 이상치 시각화 (박스플롯)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df)\n",
        "plt.title(\"Boxplot of Features\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3dqj_flHuP-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치 제거 예시\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# IQR을 활용한 이상치 제거 예시\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "filtered_data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ],
      "metadata": {
        "id": "LHfbu8UxuQBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 클러스터링 바로 적용"
      ],
      "metadata": {
        "id": "LlykWJo_vGmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Elbow Method로 최적 k 찾기\n",
        "inertia = []\n",
        "k_values = range(1, 10)\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Elbow Plot\n",
        "plt.plot(k_values, inertia, marker='o')\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4lezu9dtsKhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df[['소득', '소비']])\n",
        "\n",
        "# K-Means 클러스터링\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['클러스터'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# 결과 출력\n",
        "print(df)\n",
        "\n",
        "# 클러스터링 시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "for cluster in df['클러스터'].unique():\n",
        "    cluster_data = df[df['클러스터'] == cluster]\n",
        "    plt.scatter(cluster_data['소득'], cluster_data['소비'], label=f\"Cluster {cluster}\")\n",
        "\n",
        "plt.xlabel('소득')\n",
        "plt.ylabel('소비')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fcsCe7lXvOKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# DBSCAN 클러스터링\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # epsilon과 min_samples는 데이터에 맞게 조정\n",
        "db_clusters = dbscan.fit_predict(scaled_data)\n",
        "\n",
        "# 클러스터링 결과 시각화 (2D 데이터인 경우)\n",
        "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=db_clusters, cmap='viridis')\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sk9oLPuTsKfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-2: 만약 데이터 고차원 PCA"
      ],
      "metadata": {
        "id": "5ho_0vEKo1Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. 데이터 로드\n",
        "# 데이터는 이미 전처리된 상태라고 가정\n",
        "data = pd.read_csv(\"high_dimensional_data.csv\")\n",
        "\n",
        "# 2. 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# 3. PCA를 이용한 차원 축소\n",
        "# 주성분 수는 누적 설명력이 90% 이상이 되는 값으로 선택\n",
        "pca = PCA()\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "# 누적 설명력 계산 및 시각화\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.6, label='Individual Explained Variance')\n",
        "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative Explained Variance', color='red')\n",
        "plt.title(\"Explained Variance by PCA Components\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# 주성분 수 결정 (90% 이상의 설명력을 가지는 최소 차원 수)\n",
        "optimal_components = (cumulative_variance >= 0.90).argmax() + 1\n",
        "print(f\"Optimal number of components: {optimal_components}\")\n",
        "\n",
        "# PCA 변환 (최적 차원으로 축소)\n",
        "pca = PCA(n_components=optimal_components)\n",
        "reduced_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "# 4. 클러스터링\n",
        "## a. K-Means 클러스터링\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)  # 군집 수는 예시로 4개 설정\n",
        "kmeans_labels = kmeans.fit_predict(reduced_data)\n",
        "\n",
        "## b. DBSCAN 클러스터링\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # epsilon과 min_samples는 데이터에 따라 조정\n",
        "dbscan_labels = dbscan.fit_predict(reduced_data)\n",
        "\n",
        "# 5. 결과 평가\n",
        "## Silhouette Score 계산\n",
        "kmeans_sil_score = silhouette_score(reduced_data, kmeans_labels)\n",
        "print(f\"Silhouette Score (K-Means): {kmeans_sil_score:.2f}\")\n",
        "\n",
        "if len(set(dbscan_labels)) > 1:  # DBSCAN은 -1(노이즈) 포함 가능\n",
        "    dbscan_sil_score = silhouette_score(reduced_data, dbscan_labels)\n",
        "    print(f\"Silhouette Score (DBSCAN): {dbscan_sil_score:.2f}\")\n",
        "else:\n",
        "    print(\"DBSCAN 결과에 군집이 하나만 있거나 노이즈가 많습니다.\")\n",
        "\n",
        "# 6. 시각화 (2D PCA 데이터 기준)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# K-Means 시각화\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans_labels, cmap='viridis')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "\n",
        "# DBSCAN 시각화\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=dbscan_labels, cmap='viridis')\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0psHzi_QSPjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 코드 구성 설명\n",
        "\n",
        "* 스케일링: PCA와 클러스터링을 정확히 수행하기 위해 데이터 스케일링 필수.\n",
        "\n",
        "* PCA 변환:\n",
        "모든 주성분의 누적 설명력을 확인하고, 90% 이상 설명력을 가진 최소 차원 수를 결정.\n",
        "PCA로 데이터를 축소.\n",
        "\n",
        "* 클러스터링:\n",
        "K-Means: 사전에 군집 수를 지정. 결과를 Silhouette Score로 평가.\n",
        "\n",
        "* DBSCAN: 밀도 기반 클러스터링. 노이즈를 포함한 결과를 생성.\n",
        "\n"
      ],
      "metadata": {
        "id": "--4Sh_SYxYN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 결과 평가: Silhouette Score를 통해 각 클러스터링의 품질 평가.\n",
        "\n",
        "* 시각화:\n",
        "PCA로 축소한 2D 데이터를 기준으로 클러스터링 결과를 시각화.\n",
        "결과 해석\n",
        "\n",
        "* PCA의 주성분 개수: 적절한 차원을 선택해 차원 축소.\n",
        "클러스터링 품질: Silhouette Score로 클러스터링의 적합성 평가.\n",
        "\n",
        "* 군집 결과 시각화: 군집 구조를 직관적으로 이해."
      ],
      "metadata": {
        "id": "xA_zzVAPxlDU"
      }
    }
  ]
}